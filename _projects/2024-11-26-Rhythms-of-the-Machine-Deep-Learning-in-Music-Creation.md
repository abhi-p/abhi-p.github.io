---
title: "Rhythms of the Machine: Deep Learning in Music Creation"
collection: projects
urlslug: "music-gen-ai"
type: "Academic"
#teaserurl: 'https://www.youtube.com/watch?v=1Mg0Z7lUwNM&ab_channel=AbhishekPaul'
#start_time: 547
# demourl: "https://youtu.be/StTqXEQ2l-Y?t=35s" # [![Everything Is AWESOME](https://i.sstatic.net/q3ceS.png)](https://youtu.be/StTqXEQ2l-Y?t=35s "Everything Is AWESOME")
# **Resources:** [[Technical report](https://adivekar-utexas.github.io/files/UTCS-Deep-Learning-Final-Autonomous-agents-for-realtime-multiplayer-ice-hockey.pdf)]

permalink: /projects/2024-11-26-Rhythms-of-the-Machine-Deep-Learning-in-Music-Creation
contributors: "Abhishek Paul"
contribution: "On all aspects of the project"
date: 2024-11-26
reporturl: 'https://abhi-p.github.io/files/Music_Generation_Using_Machine_Learning.pdf'

# videourl: "https://www.youtube.com/embed/15sa6OeIWJg"
# codeurl: 'https://github.com/ARDivekar/SearchDistribute'
excerpt: "<br /> \
**Project Goal**:  The goal of this project is to create an application that allows musicians to create extensions to their input. I created and trained an LSTM-based recurrent neural network designed to model monophonic music with expressive timing and dynamics.\

<br /><br />
**Outcome:** The model successfully generated fluid extensions, maintaining the melody and rhythmic structure. The framework I developed allows for future enhancements of neural network models for music generation. This adaptable framework requires minimal code changes to use different datasets and alter model parameters for future projects. The model can use any variable length of music input to generate an extension or no input to create a fresh composition. Here are a few examples generated by the model:
 \

<br /><br />
Input: 
<br /><br />
<audio controls>
    <source src='https://abhi-p.github.io/files/monophonic_rnn/long_extension/input_melody_1.mp3' type='audio/mpeg'>
    Your browser does not support the audio element.
</audio>
<br /><br />

Generated Extensions: 
<br/><br/>

<div style='display: flex; gap: 20px; align-items: center;'>
<audio controls>
    <source src='https://abhi-p.github.io/files/monophonic_rnn/long_extension/2024-11-15_205656_06.mp3' type='audio/mpeg'>
    Your browser does not support the audio element.
</audio>

<audio controls>
    <source src='https://abhi-p.github.io/files/monophonic_rnn/long_extension/2024-11-15_205656_08.mp3' type='audio/mpeg'>
    Your browser does not support the audio element.
</audio>

<audio controls>
    <source src='https://abhi-p.github.io/files/monophonic_rnn/long_extension/2024-11-15_205656_09.mp3' type='audio/mpeg'>
    Your browser does not support the audio element.
</audio>
</div>


The model is also able to generate compositions from scratch without any input. For the outputs below, the model had no prior musical context and had to rely solely on its learned weights and predictions to construct the compositions:


<br/><br/>

<div style='display: flex; gap: 20px; align-items: center;'>
<br /><br />
<audio controls>
    <source src='https://abhi-p.github.io/files/monophonic_rnn/Fresh_Generation/2024-11-20_123309_01.mp3' type='audio/mpeg'>


    
    Your browser does not support the audio element.
</audio>
<br /><br />


<br /><br />
<audio controls>
    <source src='https://abhi-p.github.io/files/monophonic_rnn/Fresh_Generation/2024-11-20_123309_02.mp3' type='audio/mpeg'>
    Your browser does not support the audio element.
</audio>
<br /><br />


<br /><br />
<audio controls>
    <source src='https://abhi-p.github.io/files/monophonic_rnn/Fresh_Generation/2024-11-20_123309_04.mp3' type='audio/mpeg'>
    Your browser does not support the audio element.
</audio>
<br /><br />

</div>

"


---

